This is my repository. Below is the directory tree, followed by the contents of key files.

Repository root: /Users/snag/Library/Mobile Documents/iCloud~md~obsidian/Documents/UNH_Classes_Obsidian/Fall 2025/CS 722/cs-722/project_1

### DIRECTORY TREE ###
```text
project_1
├── client
│   ├── main.go
│   ├── results.txt
│   └── servers.txt
├── cloud_setup
│   ├── client
│   │   ├── main.go
│   │   ├── results.txt
│   │   └── servers.txt
│   ├── server
│   │   └── main.go
│   └── .DS_Store
├── proto
│   ├── server.pb.go
│   ├── server.proto
│   └── server_grpc.pb.go
├── results
│   ├── initial
│   │   ├── 10sec
│   │   │   ├── results_1.1.txt
│   │   │   ├── results_1.2.txt
│   │   │   ├── results_1.3.txt
│   │   │   ├── results_10.1.txt
│   │   │   ├── results_10.2.txt
│   │   │   ├── results_10.3.txt
│   │   │   ├── results_10a.1.txt
│   │   │   ├── results_10a.2.txt
│   │   │   ├── results_10a.3.txt
│   │   │   ├── results_2.1.txt
│   │   │   ├── results_2.2.txt
│   │   │   ├── results_2.3.txt
│   │   │   ├── results_2a.1.txt
│   │   │   ├── results_2a.2.txt
│   │   │   ├── results_2a.3.txt
│   │   │   ├── results_3.1.txt
│   │   │   ├── results_3.2.txt
│   │   │   ├── results_3.3.txt
│   │   │   ├── results_3a.1.txt
│   │   │   ├── results_3a.2.txt
│   │   │   ├── results_3a.3.txt
│   │   │   ├── results_4.1.txt
│   │   │   ├── results_4.2.txt
│   │   │   ├── results_4.3.txt
│   │   │   ├── results_4a.1.txt
│   │   │   ├── results_4a.2.txt
│   │   │   └── results_4a.3.txt
│   │   ├── 60sec
│   │   │   ├── results2a_1.txt
│   │   │   ├── results2a_2.txt
│   │   │   ├── results2a_3.txt
│   │   │   ├── results3a_1.txt
│   │   │   ├── results3a_2.txt
│   │   │   ├── results3a_3.txt
│   │   │   ├── results4a_1.txt
│   │   │   ├── results4a_2.txt
│   │   │   ├── results4a_3.txt
│   │   │   ├── results_1.1.txt
│   │   │   ├── results_1.2.txt
│   │   │   ├── results_1.3.txt
│   │   │   ├── results_1.4.txt
│   │   │   ├── results_2.1.txt
│   │   │   ├── results_2.2.txt
│   │   │   ├── results_2.3.txt
│   │   │   ├── results_3.1.txt
│   │   │   ├── results_3.2.txt
│   │   │   ├── results_3.3.txt
│   │   │   ├── results_4.1.txt
│   │   │   ├── results_4.2.txt
│   │   │   └── results_4.3.txt
│   │   └── throughputs_latencies.txt
│   ├── updated
│   │   └── 60sec
│   │       ├── 100_1.txt
│   │       ├── 100_2.txt
│   │       ├── 100_a1.txt
│   │       ├── 100_a2.txt
│   │       ├── 150_1.txt
│   │       ├── 150_2.txt
│   │       ├── 150_a1.txt
│   │       ├── 150_a2.txt
│   │       ├── 1_1.txt
│   │       ├── 1_2.txt
│   │       ├── 1_3.txt
│   │       ├── 25_1.txt
│   │       ├── 25_2.txt
│   │       ├── 25_a1.txt
│   │       ├── 25_a2.txt
│   │       ├── 2_1.txt
│   │       ├── 2_2.txt
│   │       ├── 2_3.txt
│   │       ├── 2_a1.txt
│   │       ├── 2_a2.txt
│   │       ├── 3_1.txt
│   │       ├── 3_2.txt
│   │       ├── 3_3.txt
│   │       ├── 4_1.txt
│   │       ├── 4_2.txt
│   │       ├── 4_3.txt
│   │       ├── 50_1.txt
│   │       ├── 50_2.txt
│   │       ├── 50_3.txt
│   │       ├── 50_a1.txt
│   │       └── 50_a2.txt
│   ├── main.go
│   └── throughputs_latencies.txt
├── server
│   ├── main.go
│   └── main_old.go
├── .DS_Store
├── buf.gen.yaml
├── buf.lock
├── buf.yaml
├── go.mod
├── go.sum
├── Project 1 Report - Derek Dong.pdf
└── project_dump.py
```
### FILES ###

[BEGIN FILE: buf.gen.yaml]
```text
version: v2
plugins:
  - local: protoc-gen-go
    out: .
    opt:
      - paths=source_relative
  - local: protoc-gen-go-grpc
    out: .
    opt:
      - paths=source_relative
  - local: protoc-gen-grpc-gateway
    out: .
    opt:
      - paths=source_relative
```
[END FILE: buf.gen.yaml]


[BEGIN FILE: buf.lock]
```text
# Generated by buf. DO NOT EDIT.
version: v2
deps:
  - name: buf.build/googleapis/googleapis
    commit: 72c8614f3bd0466ea67931ef2c43d608
    digest: b5:13efeea24e633fd45327390bdee941207a8727e96cf01affb84c1e4100fd8f48a42bbd508df11930cd2884629bafad685df1ac3111bc78cdaefcd38c9371c6b1

```
[END FILE: buf.lock]


[BEGIN FILE: buf.yaml]
```text
version: v2
name: buf.build/yourorg/myprotos
deps:
  - buf.build/googleapis/googleapis
```
[END FILE: buf.yaml]


[BEGIN FILE: client/main.go]
```text
package main

import (
	"bufio"
	"bytes"
	"encoding/json"
	"flag"
	"fmt"
	"io"
	"log"
	"math/rand"
	"net/http"
	"net/url"
	"os"
	"sync"
	"time"
)

type PingResult struct {
		isRead bool
		Start string
		End string
		Delta int64
		Key string
		Value string
}

var mu sync.Mutex 

func benchmark(servers []string, runtime int, backupReads bool, readRatio int, threads int, size int) {
	var client = &http.Client{
		Transport: &http.Transport{
			MaxIdleConns:        100,
			MaxIdleConnsPerHost: 100,
			IdleConnTimeout:     90 * time.Second,
		},
	}
	var results []PingResult
	var attemptedReads = 0
	var attemptedWrites = 0
	start := time.Now()
	duration := time.Duration(runtime) * time.Second
	for t := 0; t < threads; t++ {
		go func() {
			for time.Since(start) < duration {
				randomNumber := rand.Int() % 100
				var result PingResult
				if (randomNumber < readRatio) { // Read
					attemptedReads++
					if backupReads {
						result = pingServer(client, servers[rand.Int() % len(servers)], true, size)
					} else {
						result = pingServer(client, servers[0], true, size)
					}
				} else {
					attemptedWrites++
					result = pingServer(client, servers[0], false, size)
				}
				mu.Lock()
				if result.Value != "" { // Successful read/write
					results = append(results, result)
				}
				mu.Unlock()
			}
		}()
	}

	time.Sleep(duration + 500*time.Millisecond)

	f, err := os.Create("results.txt")
	if err != nil {
		log.Fatalf("Failed to create file: %v", err)
	}
	defer f.Close()
	var total = 0
	var totalReads = 0
	var totalWrites = 0
	var totalReadLatency = 0
	for _, result := range results {
		var opType string
		if result.isRead {
			opType = "READ"
			totalReads++
			totalReadLatency += int(result.Delta)
		} else {
			opType = "WRITE"
			totalWrites++
		}
		total += int(result.Delta)
		fmt.Fprintf(f, "%s,%s,%s,%s,%s,%dms\n", opType, result.Start, result.End, result.Key, result.Value, result.Delta)
	}
	throughput := float64(len(results)) / float64(runtime)
	latency := float64(total) / float64(len(results))
	avgReadLatency := float64(totalReadLatency) / float64(totalReads)
	fmt.Printf("Total Throughput: %f\n", throughput)
	fmt.Printf("Average Latency: %f\n", latency)
	fmt.Printf("Average Read Latency: %f\n", avgReadLatency)
	fmt.Printf("Total Reads: %d\n", totalReads)
	fmt.Printf("Total Writes: %d\n", totalWrites)
	fmt.Printf("Attempted Reads: %d\n", attemptedReads)
	fmt.Printf("Attempted Writes: %d\n", attemptedWrites)
}

func pingServer(client *http.Client, addr string, isRead bool, size int) (PingResult) {
	fmt.Printf("pinging %s\n", addr)
	start := time.Now()
	if (isRead) { // Read
		key := randomString(1)
		endpoint := fmt.Sprintf("http://%s/%s", addr, key)

		resp, err := client.Get(endpoint)
		if err != nil {
			log.Printf("HTTP request failed: %v", err)
			return PingResult {
				Value: "",
			}
		}
		defer resp.Body.Close()

		body, _ := io.ReadAll(resp.Body)
		if resp.StatusCode != http.StatusOK {
			return PingResult {
				Value: "",
			}
		}
		var respMap map[string]string
		err = json.Unmarshal(body, &respMap)
		if err != nil {
				log.Fatalf("Failed to parse JSON: %v", err)
		}
		end := time.Now()
		return PingResult {
			isRead: true,
			Start: start.Format("2006-01-02 15:04:05.000"),
			End: end.Format("2006-01-02 15:04:05.000"),
			Delta: end.Sub(start).Milliseconds(),
			Key: key,
			Value: string(respMap["value"]),
		}
	} else { // Write
		key := randomString(1)
		endpoint := fmt.Sprintf("http://%s/%s", addr, key)
		form := url.Values{}
		value := randomString(size)
		form.Set("val", value)
		resp, err := client.Post(endpoint, "application/x-www-form-urlencoded", bytes.NewBufferString(form.Encode()))
		if err != nil {
			return PingResult {
				Value: "",
			}
		}
		defer resp.Body.Close()

		io.ReadAll(resp.Body)

		if resp.StatusCode != http.StatusOK {
			return PingResult {
				Value: "",
			}
		}

		end := time.Now()
		return PingResult {
			isRead: false,
			Start: start.Format("2006-01-02 15:04:05.000"),
			End: end.Format("2006-01-02 15:04:05.000"),
			Delta: end.Sub(start).Milliseconds(),
			Key: key,
			Value: value,
		}
	}
}

const letters = "abcdefghijklmnopqrstuvwxyz"

func randomString(size int) string {
	b := make([]byte, size)
	for i := range b {
		b[i] = letters[rand.Intn(len(letters))]
	}
	return string(b)
}

func main() {
	file, err := os.Open("servers.txt")
	if err != nil {
		log.Fatalf("Error opening file: %v", err)
	}
	defer file.Close()

	servers := [3]string{}
	scanner := bufio.NewScanner(file)
	for i := 0; i < 3; i++ {
		scanner.Scan()
		servers[i] = scanner.Text()
	}

	if err := scanner.Err(); err != nil {
		log.Fatalf("Error scanning file: %v", err)
	}

	time := flag.Int("time", 60, "time to run")
	readBackups := flag.Bool("readBackup", false, "read to backups?")
	ratio := flag.Int("ratio", 100, "read to write ratio")
	threads := flag.Int("threads", 1, "number of threads")
	size := flag.Int("size", 1, "size of write")
	flag.Parse()

	benchmark(servers[:], *time, *readBackups, *ratio, *threads, *size)
}
```
[END FILE: client/main.go]


[BEGIN FILE: cloud_setup/client/main.go]
```text
package main

import (
	"bufio"
	"bytes"
	"encoding/json"
	"flag"
	"fmt"
	"io"
	"log"
	"math/rand"
	"net/http"
	"net/url"
	"os"
	"sync"
	"time"
)

// Asked chat the best way to write to a file my results, and because there might be threading issues, it suggested using a dedicated writer goRoutine to do so concurrently.
type PingResult struct {
		isRead bool
		Start string
		End string
		Delta int64
		Key string
		Value string
}

var mu sync.Mutex 

func benchmark(servers []string, runtime int, backupReads bool, readRatio int, threads int, size int) {
	var client = &http.Client{
		Transport: &http.Transport{
			MaxIdleConns:        100,
			MaxIdleConnsPerHost: 100,
			IdleConnTimeout:     90 * time.Second,
		},
	}
	var results []PingResult
	var attemptedReads = 0
	var attemptedWrites = 0
	start := time.Now()
	duration := time.Duration(runtime) * time.Second
	for t := 0; t < threads; t++ {
		go func() {
			for time.Since(start) < duration {
				randomNumber := rand.Int() % 100
				var result PingResult
				if (randomNumber < readRatio) { // Read
					attemptedReads++
					if backupReads {
						result = pingServer(client, servers[rand.Int() % len(servers)], true, size)
					} else {
						result = pingServer(client, servers[0], true, size)
					}
				} else {
					attemptedWrites++
					result = pingServer(client, servers[0], false, size)
				}
				mu.Lock()
				if result.Value != "" { // Successful read/write
					results = append(results, result)
				}
				mu.Unlock()
			}
		}()
	}

	time.Sleep(duration + 500*time.Millisecond)

	f, err := os.Create("results.txt")
	if err != nil {
		log.Fatalf("Failed to create file: %v", err)
	}
	defer f.Close()
	var total = 0
	var totalReads = 0
	var totalWrites = 0
	var totalReadLatency = 0
	for _, result := range results {
		var opType string
		if result.isRead {
			opType = "READ"
			totalReads++
			totalReadLatency += int(result.Delta)
		} else {
			opType = "WRITE"
			totalWrites++
		}
		total += int(result.Delta)
		fmt.Fprintf(f, "%s,%s,%s,%s,%s,%dms\n", opType, result.Start, result.End, result.Key, result.Value, result.Delta)
	}
	throughput := float64(len(results)) / float64(runtime)
	latency := float64(total) / float64(len(results))
	avgReadLatency := float64(totalReadLatency) / float64(totalReads)
	fmt.Printf("Total Throughput: %f\n", throughput)
	fmt.Printf("Average Latency: %f\n", latency)
	fmt.Printf("Average Read Latency: %f\n", avgReadLatency)
	fmt.Printf("Total Reads: %d\n", totalReads)
	fmt.Printf("Total Writes: %d\n", totalWrites)
	fmt.Printf("Attempted Reads: %d\n", attemptedReads)
	fmt.Printf("Attempted Writes: %d\n", attemptedWrites)
}

func pingServer(client *http.Client, addr string, isRead bool, size int) (PingResult) {
	fmt.Printf("pinging %s\n", addr)
	start := time.Now()
	if (isRead) { // Read
		key := randomString(1)
		endpoint := fmt.Sprintf("http://%s/%s", addr, key)

		resp, err := client.Get(endpoint)
		if err != nil {
			log.Printf("HTTP request failed: %v", err)
			return PingResult {
				Value: "",
			}
		}
		defer resp.Body.Close()

		body, _ := io.ReadAll(resp.Body)
		if resp.StatusCode != http.StatusOK {
			return PingResult {
				Value: "",
			}
		}
		var respMap map[string]string
		err = json.Unmarshal(body, &respMap)
		if err != nil {
				log.Fatalf("Failed to parse JSON: %v", err)
		}
		end := time.Now()
		return PingResult {
			isRead: true,
			Start: start.Format("2006-01-02 15:04:05.000"),
			End: end.Format("2006-01-02 15:04:05.000"),
			Delta: end.Sub(start).Milliseconds(),
			Key: key,
			Value: string(respMap["value"]),
		}
	} else { // Write
		key := randomString(1)
		endpoint := fmt.Sprintf("http://%s/%s", addr, key)
		form := url.Values{}
		value := randomString(size)
		form.Set("val", value)
		resp, err := client.Post(endpoint, "application/x-www-form-urlencoded", bytes.NewBufferString(form.Encode()))
		if err != nil {
			return PingResult {
				Value: "",
			}
		}
		defer resp.Body.Close()

		io.ReadAll(resp.Body)

		if resp.StatusCode != http.StatusOK {
			return PingResult {
				Value: "",
			}
		}

		end := time.Now()
		return PingResult {
			isRead: false,
			Start: start.Format("2006-01-02 15:04:05.000"),
			End: end.Format("2006-01-02 15:04:05.000"),
			Delta: end.Sub(start).Milliseconds(),
			Key: key,
			Value: value,
		}
	}
}

const letters = "abcdefghijklmnopqrstuvwxyz"

func randomString(size int) string {
	b := make([]byte, size)
	for i := range b {
		b[i] = letters[rand.Intn(len(letters))]
	}
	return string(b)
}

func main() {
	file, err := os.Open("servers.txt")
	if err != nil {
		log.Fatalf("Error opening file: %v", err)
	}
	defer file.Close()

	servers := [3]string{}
	scanner := bufio.NewScanner(file)
	for i := 0; i < 3; i++ {
		scanner.Scan()
		servers[i] = scanner.Text()
	}

	if err := scanner.Err(); err != nil {
		log.Fatalf("Error scanning file: %v", err)
	}

	time := flag.Int("time", 60, "time to run")
	readBackups := flag.Bool("readBackup", false, "read to backups?")
	ratio := flag.Int("ratio", 100, "read to write ratio")
	threads := flag.Int("threads", 1, "number of threads")
	size := flag.Int("size", 1, "size of write")
	flag.Parse()

	benchmark(servers[:], *time, *readBackups, *ratio, *threads, *size)
}
```
[END FILE: cloud_setup/client/main.go]


[BEGIN FILE: cloud_setup/server/main.go]
```text
package main

import (
	"context"
	"encoding/json"
	"flag" // For testing purposes from chat, so I can distinguish on my local machine the 2 servers
	"log"
	"net"
	"net/http"
	"sync"
	"runtime"
	"time"
	"container/heap"

	pb "project_1/proto"

	"google.golang.org/grpc"
	"google.golang.org/grpc/credentials/insecure"
)

// Chat kept bitching to use a struct instead of leaving these as global variables so I decided to go with it eventually.
// I guess chat is good for trying to make code look neat and with good programming practices
// In the end I think it is easier to compartimentalize my code with this structure, so I'll have to give it to chat here
// But yeah hopefully this is an appropriate use of chat
// (also if I don't do this chat keeps defaulting to using this so its easier to see its suggestions than trying to translate my previous way with global variables.)
type server struct {
	pb.UnimplementedReplicationServiceServer
	hTTPport string // Decided to use the httpport as the ID that the spec wants
	gRPCport string
	is_primary bool
	data_store map[string]string
	lsn int64
	command_log map[int64]*pb.Operation
	addr []string
	conns []*grpc.ClientConn
	servers []pb.ReplicationServiceClient

	// This is probably the most important part of the code that Chat has helped me with (besides getting started in the first place)
	// I've realized that mutexes and ensuring we don't have race conditions is probably necessary to make sure that we have
	// mutual exclusion in critical parts of code. At least I think that's good, although ig we can just wait forever until
	// our values are applied. But mutexes are still good to make sure that the values are applied in order if we have writes
	// and reads to the same value. So I guess in the end this is just a good idea.
	mu   sync.Mutex
	cond *sync.Cond
	commitHeap *CommitHeap
	completedLsn int64
}

type CommitHeap []*pb.Commit

func (h CommitHeap) Len() int           { return len(h) }
func (h CommitHeap) Less(i, j int) bool { return h[i].Lsn < h[j].Lsn } // smallest LSN first
func (h CommitHeap) Swap(i, j int)      { h[i], h[j] = h[j], h[i] }

func (h *CommitHeap) Push(x any) {
    *h = append(*h, x.(*pb.Commit))
}

func (h *CommitHeap) Pop() any {
    old := *h
    n := len(old)
    x := old[n-1]
    *h = old[:n-1]
    return x
}

var totalReads = 0
var totalWrites = 0
var debug = true

// Chat has made me aware that I need to actually do an HTTP server, not the gRPC
func (s *server) Read(w http.ResponseWriter, r *http.Request) {
	key := r.URL.Path[1:]
	if debug {
		totalReads += 1
		log.Printf("total reads: %d", totalReads)
		log.Printf("Received READ request for key %s: ", key)
	}

	if s.is_primary {
		op := pb.Operation {
			Key: key,
			Val: "",
			IsRead: true,
			Applied: false,
		}
		commit := s.replicateToBackups(&op)
		s.CommitOperation(context.Background(), commit)
		s.mu.Lock()
		for !s.command_log[commit.Lsn].Applied { // Wait for applied
			s.cond.Wait()
		}
		s.mu.Unlock()
		s.CommitToBackups(commit)

		if debug {
			log.Printf("DEBUG: finishing the read")
		}
	}

	s.mu.Lock()
	value, ok := s.data_store[key]
	s.mu.Unlock()

	if debug {
		log.Printf("DEBUG: After broadcast")
	}
	if ok {
		if debug {
			log.Printf("DEBUG: Found it")
		}
		resp := map[string]string{"key": key, "value": value}
		json.NewEncoder(w).Encode(resp)
	} else {
		if debug {
		log.Printf("DEBUG: Didnt find it")
		}
		http.Error(w, "not found", http.StatusNotFound)
		return
	}
}

func (s *server) Write(w http.ResponseWriter, r *http.Request) {
	if !s.is_primary {
		http.Error(w, "cannot write to backup", http.StatusNotFound)
		return
	}
	key := r.URL.Path[1:]
	val := r.FormValue("val")
	if debug {
		totalWrites++
		log.Printf("total writes: %d", totalWrites)
		log.Printf("Received WRITE request for key %s value %s: ", key, val)
	}
	if val == "" {
		http.Error(w, "missing val field", http.StatusBadRequest)
		return
	}

	op := pb.Operation {
		Key: key,
		Val: val,
		IsRead: false,
		Applied: false,
	}
	commit := s.replicateToBackups(&op)
	s.CommitOperation(context.Background(), commit)
	s.mu.Lock()
	for !s.command_log[commit.Lsn].Applied { // Wait for applied
		s.cond.Wait()
	}
	s.mu.Unlock()
	s.CommitToBackups(commit)

	resp := map[string]string{"status": "ok"}
	json.NewEncoder(w).Encode(resp)
}

// This is from chat as trying to figure out how to do the quorum majority.
// I think the main line is the ack, err := backupClient line, as the context.Background() is the main thing that is needed for concurrency here.
func (s *server) replicateToBackups(op *pb.Operation) *pb.Commit {
	// This is standard for quorum formula
	quorum := len(s.servers) / 2 + 1
	acks := 1
	
	// Ok fuck chat i think imma just code myself i got this. I think this is the way to go about concurrency
	// but making sure everything is in order. The most important thing is that we do NOT have duplicate LSNs.
	// So I just need to make it so that we lock only the lsn increment and setting the accept for the backups
	// to get that accept request. I think this logically makes sense, it'll allow concurrent requests but
	// make sure there is never a duplicate LSN. I think we're not worried about error, just let it wait forever
	s.mu.Lock()
	accept := pb.Accept {
		Op: op,
		Lsn: s.lsn,
	}
	s.command_log[s.lsn] = op
	commit := pb.Commit {
		Lsn: s.lsn,
	}
	if debug {
		log.Printf("Debug LSN Number: %d\n", s.lsn)
	}
	s.lsn++
	s.mu.Unlock()

	ackCh := make(chan bool, len(s.servers))

	// Going through every backup
	for _, server := range s.servers {
		go func(server pb.ReplicationServiceClient) {
			// This is the important part that tells every backup to replicate the operation
			ack, err := server.ReplicateOperation(context.Background(), &accept)
			ackCh <- (err == nil && ack != nil && ack.Success)
		}(server)
	}

	for {
		if <-ackCh {
				acks++
		}
		if acks >= quorum {
			if debug {
				log.Println("DEBUG: weve got quorum")
			}
			return &commit // quorum reached, return immediately
		}
	}
}


// I took chat's original implementation and modified it to the workflow defined in the specs
// The function header was assisted by chat (for syntax), but then I figured out how to do the rest myself.
// I'm realizing go really likes them pointers (or perhaps the way chat wanted me to do it does it that way)
// Anyway, a lot of the work is just me figuring out how to get the correct types passed and returned.
func (s *server) ReplicateOperation(ctx context.Context, accept *pb.Accept) (*pb.Ack, error) {
	s.mu.Lock()
	s.command_log[accept.Lsn] = accept.Op
	s.mu.Unlock()
	s.cond.Broadcast()
	if debug {
		log.Printf("DEBUG: replicating operation\n")
	}

	return &pb.Ack {
		Id: s.hTTPport,
		Lsn: accept.Lsn,
		Success: true,
	}, nil
}

func (s *server) CommitToBackups(commit *pb.Commit) {
	if debug {
		log.Printf("DEBUG: Committing to backups")
	}
	for _, server := range s.servers {
		go func(server pb.ReplicationServiceClient) {
			server.CommitOperation(context.Background(), commit)
		}(server)
	}
}

func (s *server) CommitOperation(ctx context.Context, commit *pb.Commit) (*pb.Empty, error) {
	s.mu.Lock()
	heap.Push(s.commitHeap, commit)
	s.cond.Broadcast() // wake up the applier
	s.mu.Unlock()
	if debug {
		log.Printf("DEBUG: commit operation: %d\n", commit.Lsn)
	}
	return &pb.Empty{}, nil
}

// This is copied from program 0
func startServer(httpport string, grpcport string, primary bool, backups []string) {
	// Chat's thing for initializing our new server so that we can hold our server data
	srv := &server{
		hTTPport:         httpport,
		gRPCport: 			grpcport,
		is_primary:  primary,
		lsn:        0,
		data_store:  make(map[string]string),
		command_log: make(map[int64]*pb.Operation),
		addr:  backups,
    conns:       []*grpc.ClientConn{},
    servers:     []pb.ReplicationServiceClient{},
		completedLsn: 1,
	}
	srv.commitHeap = &CommitHeap{}
	heap.Init(srv.commitHeap)
	srv.command_log[0] = &pb.Operation{Applied: true}
	srv.lsn = 1
	// This is the part where we actually initialize the chat suggested mutexes and conds.
	srv.cond = sync.NewCond(&srv.mu)
	log.Printf("Server listening on HTTP port %s gRPC port %s (primary=%v)", httpport, grpcport, primary)
	go func() {
    for {
        log.Printf("Number of goroutines: %d", runtime.NumGoroutine())
				time.Sleep(10 * time.Second)
    }
	}()
	go func() { // Function to constantly process commits
		for {
			srv.mu.Lock()
			for srv.commitHeap.Len() == 0 || (*(srv.commitHeap))[0].Lsn != srv.completedLsn {
					srv.cond.Wait()
			}
			commit := heap.Pop(srv.commitHeap).(*pb.Commit)

			op, ok := srv.command_log[commit.Lsn]
			for !ok || op == nil {
				log.Printf("Heyy")
				srv.cond.Wait()
				op, ok = srv.command_log[commit.Lsn]
			}
			if !op.IsRead {
				if debug {
					log.Printf("Applying key %s value %s to HTTP %s", op.Key, op.Val, srv.hTTPport)
				}
				srv.data_store[op.Key] = op.Val
			}
			srv.command_log[commit.Lsn].Applied = true
			srv.completedLsn++
			srv.cond.Broadcast()
			srv.mu.Unlock()
		}
	}()
	if (srv.is_primary) {
		// So last time this was where I lost most of my points, cuz I kept connecting and disconnecting.
		// Anyways I wanted to know if the conn itself or the New_ServiceClient would be the majority of the computation.
		// Chat says that the conn itself is most as that is the actual gRPC connection, which I think does make sense.
		// Anyways I didn't actually chat this code, just needed clarification. I'm just writing to make sure I'm not cheating
		// This is mostly copied from program_0 and then modified for my needs this time.
		for _, backup := range backups {
			conn, err := grpc.NewClient(backup, grpc.WithTransportCredentials(insecure.NewCredentials()))
			if err != nil {
				log.Fatalf("Failed to connect: %v", err)
			}
			srv.conns = append(srv.conns, conn)
			srv.servers = append(srv.servers, pb.NewReplicationServiceClient(conn))
		}
	} else {
		go func() {
			grpcServer := grpc.NewServer() // Creates a gRPC server object that can use the gRPC protocol
			pb.RegisterReplicationServiceServer(grpcServer, srv)
			lis, err := net.Listen("tcp", "0.0.0.0:" + grpcport)

			if err != nil { // Error checking, don't even need ChatGPT to understand this now, im a pro
				log.Fatalf("Failed to listen: %v", err)
			}
			if err := grpcServer.Serve(lis); err != nil {
				log.Fatalf("Failed to serve: %v", err)
			}
		}()
	}

	// Chat's thing for HTTP client-facing
	http.HandleFunc("/", func(w http.ResponseWriter, r *http.Request) {
		switch r.Method {
		case http.MethodGet:
			srv.Read(w, r)
		case http.MethodPost:
			srv.Write(w, r)
		default:
			http.Error(w, "unsupported method", http.StatusMethodNotAllowed)
		}
	})
	log.Fatal(http.ListenAndServe("0.0.0.0:" + httpport, nil))
}

func main() {
	httpport := flag.String("http", "8080", "http server port")
	grpcport := flag.String("grpc", "50051", "grpc server port")
	primary := flag.Bool("primary", false, "is primary node")
	backups := []string{"ip:50051", "ip:50051"} // Chat suggested putting this here, ig its better for programming practice to put it in an easy spot to modify
	flag.Parse()
	startServer(*httpport, *grpcport, *primary, backups)
}
```
[END FILE: cloud_setup/server/main.go]


[BEGIN FILE: go.mod]
```text
module project_1

go 1.25.0

require (
	github.com/grpc-ecosystem/grpc-gateway/v2 v2.27.3
	google.golang.org/genproto/googleapis/api v0.0.0-20250929231259-57b25ae835d4
	google.golang.org/grpc v1.75.1
	google.golang.org/protobuf v1.36.10
)

require (
	golang.org/x/net v0.41.0 // indirect
	golang.org/x/sys v0.33.0 // indirect
	golang.org/x/text v0.29.0 // indirect
	google.golang.org/genproto/googleapis/rpc v0.0.0-20250929231259-57b25ae835d4 // indirect
)

```
[END FILE: go.mod]


[BEGIN FILE: go.sum]
```text
github.com/go-logr/logr v1.4.3 h1:CjnDlHq8ikf6E492q6eKboGOC0T8CDaOvkHCIg8idEI=
github.com/go-logr/logr v1.4.3/go.mod h1:9T104GzyrTigFIr8wt5mBrctHMim0Nb2HLGrmQ40KvY=
github.com/go-logr/stdr v1.2.2 h1:hSWxHoqTgW2S2qGc0LTAI563KZ5YKYRhT3MFKZMbjag=
github.com/go-logr/stdr v1.2.2/go.mod h1:mMo/vtBO5dYbehREoey6XUKy/eSumjCCveDpRre4VKE=
github.com/golang/protobuf v1.5.4 h1:i7eJL8qZTpSEXOPTxNKhASYpMn+8e5Q6AdndVa1dWek=
github.com/golang/protobuf v1.5.4/go.mod h1:lnTiLA8Wa4RWRcIUkrtSVa5nRhsEGBg48fD6rSs7xps=
github.com/google/go-cmp v0.7.0 h1:wk8382ETsv4JYUZwIsn6YpYiWiBsYLSJiTsyBybVuN8=
github.com/google/go-cmp v0.7.0/go.mod h1:pXiqmnSA92OHEEa9HXL2W4E7lf9JzCmGVUdgjX3N/iU=
github.com/google/uuid v1.6.0 h1:NIvaJDMOsjHA8n1jAhLSgzrAzy1Hgr+hNrb57e+94F0=
github.com/google/uuid v1.6.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=
github.com/grpc-ecosystem/grpc-gateway/v2 v2.27.3 h1:NmZ1PKzSTQbuGHw9DGPFomqkkLWMC+vZCkfs+FHv1Vg=
github.com/grpc-ecosystem/grpc-gateway/v2 v2.27.3/go.mod h1:zQrxl1YP88HQlA6i9c63DSVPFklWpGX4OWAc9bFuaH4=
go.opentelemetry.io/auto/sdk v1.1.0 h1:cH53jehLUN6UFLY71z+NDOiNJqDdPRaXzTel0sJySYA=
go.opentelemetry.io/auto/sdk v1.1.0/go.mod h1:3wSPjt5PWp2RhlCcmmOial7AvC4DQqZb7a7wCow3W8A=
go.opentelemetry.io/otel v1.37.0 h1:9zhNfelUvx0KBfu/gb+ZgeAfAgtWrfHJZcAqFC228wQ=
go.opentelemetry.io/otel v1.37.0/go.mod h1:ehE/umFRLnuLa/vSccNq9oS1ErUlkkK71gMcN34UG8I=
go.opentelemetry.io/otel/metric v1.37.0 h1:mvwbQS5m0tbmqML4NqK+e3aDiO02vsf/WgbsdpcPoZE=
go.opentelemetry.io/otel/metric v1.37.0/go.mod h1:04wGrZurHYKOc+RKeye86GwKiTb9FKm1WHtO+4EVr2E=
go.opentelemetry.io/otel/sdk v1.37.0 h1:ItB0QUqnjesGRvNcmAcU0LyvkVyGJ2xftD29bWdDvKI=
go.opentelemetry.io/otel/sdk v1.37.0/go.mod h1:VredYzxUvuo2q3WRcDnKDjbdvmO0sCzOvVAiY+yUkAg=
go.opentelemetry.io/otel/sdk/metric v1.37.0 h1:90lI228XrB9jCMuSdA0673aubgRobVZFhbjxHHspCPc=
go.opentelemetry.io/otel/sdk/metric v1.37.0/go.mod h1:cNen4ZWfiD37l5NhS+Keb5RXVWZWpRE+9WyVCpbo5ps=
go.opentelemetry.io/otel/trace v1.37.0 h1:HLdcFNbRQBE2imdSEgm/kwqmQj1Or1l/7bW6mxVK7z4=
go.opentelemetry.io/otel/trace v1.37.0/go.mod h1:TlgrlQ+PtQO5XFerSPUYG0JSgGyryXewPGyayAWSBS0=
golang.org/x/net v0.41.0 h1:vBTly1HeNPEn3wtREYfy4GZ/NECgw2Cnl+nK6Nz3uvw=
golang.org/x/net v0.41.0/go.mod h1:B/K4NNqkfmg07DQYrbwvSluqCJOOXwUjeb/5lOisjbA=
golang.org/x/sys v0.33.0 h1:q3i8TbbEz+JRD9ywIRlyRAQbM0qF7hu24q3teo2hbuw=
golang.org/x/sys v0.33.0/go.mod h1:BJP2sWEmIv4KK5OTEluFJCKSidICx8ciO85XgH3Ak8k=
golang.org/x/text v0.26.0 h1:P42AVeLghgTYr4+xUnTRKDMqpar+PtX7KWuNQL21L8M=
golang.org/x/text v0.26.0/go.mod h1:QK15LZJUUQVJxhz7wXgxSy/CJaTFjd0G+YLonydOVQA=
golang.org/x/text v0.29.0 h1:1neNs90w9YzJ9BocxfsQNHKuAT4pkghyXc4nhZ6sJvk=
golang.org/x/text v0.29.0/go.mod h1:7MhJOA9CD2qZyOKYazxdYMF85OwPdEr9jTtBpO7ydH4=
gonum.org/v1/gonum v0.16.0 h1:5+ul4Swaf3ESvrOnidPp4GZbzf0mxVQpDCYUQE7OJfk=
gonum.org/v1/gonum v0.16.0/go.mod h1:fef3am4MQ93R2HHpKnLk4/Tbh/s0+wqD5nfa6Pnwy4E=
google.golang.org/genproto/googleapis/api v0.0.0-20250929231259-57b25ae835d4 h1:8XJ4pajGwOlasW+L13MnEGA8W4115jJySQtVfS2/IBU=
google.golang.org/genproto/googleapis/api v0.0.0-20250929231259-57b25ae835d4/go.mod h1:NnuHhy+bxcg30o7FnVAZbXsPHUDQ9qKWAQKCD7VxFtk=
google.golang.org/genproto/googleapis/rpc v0.0.0-20250707201910-8d1bb00bc6a7 h1:pFyd6EwwL2TqFf8emdthzeX+gZE1ElRq3iM8pui4KBY=
google.golang.org/genproto/googleapis/rpc v0.0.0-20250707201910-8d1bb00bc6a7/go.mod h1:qQ0YXyHHx3XkvlzUtpXDkS29lDSafHMZBAZDc03LQ3A=
google.golang.org/genproto/googleapis/rpc v0.0.0-20250929231259-57b25ae835d4 h1:i8QOKZfYg6AbGVZzUAY3LrNWCKF8O6zFisU9Wl9RER4=
google.golang.org/genproto/googleapis/rpc v0.0.0-20250929231259-57b25ae835d4/go.mod h1:HSkG/KdJWusxU1F6CNrwNDjBMgisKxGnc5dAZfT0mjQ=
google.golang.org/grpc v1.75.0 h1:+TW+dqTd2Biwe6KKfhE5JpiYIBWq865PhKGSXiivqt4=
google.golang.org/grpc v1.75.0/go.mod h1:JtPAzKiq4v1xcAB2hydNlWI2RnF85XXcV0mhKXr2ecQ=
google.golang.org/grpc v1.75.1 h1:/ODCNEuf9VghjgO3rqLcfg8fiOP0nSluljWFlDxELLI=
google.golang.org/grpc v1.75.1/go.mod h1:JtPAzKiq4v1xcAB2hydNlWI2RnF85XXcV0mhKXr2ecQ=
google.golang.org/protobuf v1.36.6 h1:z1NpPI8ku2WgiWnf+t9wTPsn6eP1L7ksHUlkfLvd9xY=
google.golang.org/protobuf v1.36.6/go.mod h1:jduwjTPXsFjZGTmRluh+L6NjiWu7pchiJ2/5YcXBHnY=
google.golang.org/protobuf v1.36.10 h1:AYd7cD/uASjIL6Q9LiTjz8JLcrh/88q5UObnmY3aOOE=
google.golang.org/protobuf v1.36.10/go.mod h1:HTf+CrKn2C3g5S8VImy6tdcUvCska2kB7j23XfzDpco=

```
[END FILE: go.sum]


[BEGIN FILE: proto/server.pb.go]
```text
// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.10
// 	protoc        (unknown)
// source: proto/server.proto

package serverrep

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// Chat suggested making a structure to help act as the Log with LSNs. I think I can just do a hash table?
// Update, for the quorum, chat suggested moving this to this proto file, as we are communicating between servers
type Operation struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Key           string                 `protobuf:"bytes,1,opt,name=key,proto3" json:"key,omitempty"`
	Val           string                 `protobuf:"bytes,2,opt,name=val,proto3" json:"val,omitempty"`
	IsRead        bool                   `protobuf:"varint,3,opt,name=isRead,proto3" json:"isRead,omitempty"`
	Applied       bool                   `protobuf:"varint,4,opt,name=applied,proto3" json:"applied,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Operation) Reset() {
	*x = Operation{}
	mi := &file_proto_server_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Operation) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Operation) ProtoMessage() {}

func (x *Operation) ProtoReflect() protoreflect.Message {
	mi := &file_proto_server_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Operation.ProtoReflect.Descriptor instead.
func (*Operation) Descriptor() ([]byte, []int) {
	return file_proto_server_proto_rawDescGZIP(), []int{0}
}

func (x *Operation) GetKey() string {
	if x != nil {
		return x.Key
	}
	return ""
}

func (x *Operation) GetVal() string {
	if x != nil {
		return x.Val
	}
	return ""
}

func (x *Operation) GetIsRead() bool {
	if x != nil {
		return x.IsRead
	}
	return false
}

func (x *Operation) GetApplied() bool {
	if x != nil {
		return x.Applied
	}
	return false
}

// This is the ACK part for communication that chat has suggested making.
type Ack struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Id            string                 `protobuf:"bytes,1,opt,name=id,proto3" json:"id,omitempty"`
	Lsn           int64                  `protobuf:"varint,2,opt,name=lsn,proto3" json:"lsn,omitempty"`
	Success       bool                   `protobuf:"varint,3,opt,name=success,proto3" json:"success,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Ack) Reset() {
	*x = Ack{}
	mi := &file_proto_server_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Ack) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Ack) ProtoMessage() {}

func (x *Ack) ProtoReflect() protoreflect.Message {
	mi := &file_proto_server_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Ack.ProtoReflect.Descriptor instead.
func (*Ack) Descriptor() ([]byte, []int) {
	return file_proto_server_proto_rawDescGZIP(), []int{1}
}

func (x *Ack) GetId() string {
	if x != nil {
		return x.Id
	}
	return ""
}

func (x *Ack) GetLsn() int64 {
	if x != nil {
		return x.Lsn
	}
	return 0
}

func (x *Ack) GetSuccess() bool {
	if x != nil {
		return x.Success
	}
	return false
}

// I actually didn't chat this les go
type Accept struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Op            *Operation             `protobuf:"bytes,1,opt,name=op,proto3" json:"op,omitempty"`
	Lsn           int64                  `protobuf:"varint,2,opt,name=lsn,proto3" json:"lsn,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Accept) Reset() {
	*x = Accept{}
	mi := &file_proto_server_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Accept) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Accept) ProtoMessage() {}

func (x *Accept) ProtoReflect() protoreflect.Message {
	mi := &file_proto_server_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Accept.ProtoReflect.Descriptor instead.
func (*Accept) Descriptor() ([]byte, []int) {
	return file_proto_server_proto_rawDescGZIP(), []int{2}
}

func (x *Accept) GetOp() *Operation {
	if x != nil {
		return x.Op
	}
	return nil
}

func (x *Accept) GetLsn() int64 {
	if x != nil {
		return x.Lsn
	}
	return 0
}

type Commit struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Lsn           int64                  `protobuf:"varint,1,opt,name=lsn,proto3" json:"lsn,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Commit) Reset() {
	*x = Commit{}
	mi := &file_proto_server_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Commit) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Commit) ProtoMessage() {}

func (x *Commit) ProtoReflect() protoreflect.Message {
	mi := &file_proto_server_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Commit.ProtoReflect.Descriptor instead.
func (*Commit) Descriptor() ([]byte, []int) {
	return file_proto_server_proto_rawDescGZIP(), []int{3}
}

func (x *Commit) GetLsn() int64 {
	if x != nil {
		return x.Lsn
	}
	return 0
}

type Empty struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Empty) Reset() {
	*x = Empty{}
	mi := &file_proto_server_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Empty) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Empty) ProtoMessage() {}

func (x *Empty) ProtoReflect() protoreflect.Message {
	mi := &file_proto_server_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Empty.ProtoReflect.Descriptor instead.
func (*Empty) Descriptor() ([]byte, []int) {
	return file_proto_server_proto_rawDescGZIP(), []int{4}
}

var File_proto_server_proto protoreflect.FileDescriptor

const file_proto_server_proto_rawDesc = "" +
	"\n" +
	"\x12proto/server.proto\x12\x06server\"a\n" +
	"\tOperation\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x10\n" +
	"\x03val\x18\x02 \x01(\tR\x03val\x12\x16\n" +
	"\x06isRead\x18\x03 \x01(\bR\x06isRead\x12\x18\n" +
	"\aapplied\x18\x04 \x01(\bR\aapplied\"A\n" +
	"\x03Ack\x12\x0e\n" +
	"\x02id\x18\x01 \x01(\tR\x02id\x12\x10\n" +
	"\x03lsn\x18\x02 \x01(\x03R\x03lsn\x12\x18\n" +
	"\asuccess\x18\x03 \x01(\bR\asuccess\"=\n" +
	"\x06Accept\x12!\n" +
	"\x02op\x18\x01 \x01(\v2\x11.server.OperationR\x02op\x12\x10\n" +
	"\x03lsn\x18\x02 \x01(\x03R\x03lsn\"\x1a\n" +
	"\x06Commit\x12\x10\n" +
	"\x03lsn\x18\x01 \x01(\x03R\x03lsn\"\a\n" +
	"\x05Empty2y\n" +
	"\x12ReplicationService\x121\n" +
	"\x12ReplicateOperation\x12\x0e.server.Accept\x1a\v.server.Ack\x120\n" +
	"\x0fCommitOperation\x12\x0e.server.Commit\x1a\r.server.EmptyB\x12Z\x10proto/;serverrepb\x06proto3"

var (
	file_proto_server_proto_rawDescOnce sync.Once
	file_proto_server_proto_rawDescData []byte
)

func file_proto_server_proto_rawDescGZIP() []byte {
	file_proto_server_proto_rawDescOnce.Do(func() {
		file_proto_server_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_proto_server_proto_rawDesc), len(file_proto_server_proto_rawDesc)))
	})
	return file_proto_server_proto_rawDescData
}

var file_proto_server_proto_msgTypes = make([]protoimpl.MessageInfo, 5)
var file_proto_server_proto_goTypes = []any{
	(*Operation)(nil), // 0: server.Operation
	(*Ack)(nil),       // 1: server.Ack
	(*Accept)(nil),    // 2: server.Accept
	(*Commit)(nil),    // 3: server.Commit
	(*Empty)(nil),     // 4: server.Empty
}
var file_proto_server_proto_depIdxs = []int32{
	0, // 0: server.Accept.op:type_name -> server.Operation
	2, // 1: server.ReplicationService.ReplicateOperation:input_type -> server.Accept
	3, // 2: server.ReplicationService.CommitOperation:input_type -> server.Commit
	1, // 3: server.ReplicationService.ReplicateOperation:output_type -> server.Ack
	4, // 4: server.ReplicationService.CommitOperation:output_type -> server.Empty
	3, // [3:5] is the sub-list for method output_type
	1, // [1:3] is the sub-list for method input_type
	1, // [1:1] is the sub-list for extension type_name
	1, // [1:1] is the sub-list for extension extendee
	0, // [0:1] is the sub-list for field type_name
}

func init() { file_proto_server_proto_init() }
func file_proto_server_proto_init() {
	if File_proto_server_proto != nil {
		return
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_proto_server_proto_rawDesc), len(file_proto_server_proto_rawDesc)),
			NumEnums:      0,
			NumMessages:   5,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_proto_server_proto_goTypes,
		DependencyIndexes: file_proto_server_proto_depIdxs,
		MessageInfos:      file_proto_server_proto_msgTypes,
	}.Build()
	File_proto_server_proto = out.File
	file_proto_server_proto_goTypes = nil
	file_proto_server_proto_depIdxs = nil
}

```
[END FILE: proto/server.pb.go]


[BEGIN FILE: proto/server_grpc.pb.go]
```text
// Code generated by protoc-gen-go-grpc. DO NOT EDIT.
// versions:
// - protoc-gen-go-grpc v1.5.1
// - protoc             (unknown)
// source: proto/server.proto

package serverrep

import (
	context "context"
	grpc "google.golang.org/grpc"
	codes "google.golang.org/grpc/codes"
	status "google.golang.org/grpc/status"
)

// This is a compile-time assertion to ensure that this generated file
// is compatible with the grpc package it is being compiled against.
// Requires gRPC-Go v1.64.0 or later.
const _ = grpc.SupportPackageIsVersion9

const (
	ReplicationService_ReplicateOperation_FullMethodName = "/server.ReplicationService/ReplicateOperation"
	ReplicationService_CommitOperation_FullMethodName    = "/server.ReplicationService/CommitOperation"
)

// ReplicationServiceClient is the client API for ReplicationService service.
//
// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://pkg.go.dev/google.golang.org/grpc/?tab=doc#ClientConn.NewStream.
type ReplicationServiceClient interface {
	ReplicateOperation(ctx context.Context, in *Accept, opts ...grpc.CallOption) (*Ack, error)
	CommitOperation(ctx context.Context, in *Commit, opts ...grpc.CallOption) (*Empty, error)
}

type replicationServiceClient struct {
	cc grpc.ClientConnInterface
}

func NewReplicationServiceClient(cc grpc.ClientConnInterface) ReplicationServiceClient {
	return &replicationServiceClient{cc}
}

func (c *replicationServiceClient) ReplicateOperation(ctx context.Context, in *Accept, opts ...grpc.CallOption) (*Ack, error) {
	cOpts := append([]grpc.CallOption{grpc.StaticMethod()}, opts...)
	out := new(Ack)
	err := c.cc.Invoke(ctx, ReplicationService_ReplicateOperation_FullMethodName, in, out, cOpts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *replicationServiceClient) CommitOperation(ctx context.Context, in *Commit, opts ...grpc.CallOption) (*Empty, error) {
	cOpts := append([]grpc.CallOption{grpc.StaticMethod()}, opts...)
	out := new(Empty)
	err := c.cc.Invoke(ctx, ReplicationService_CommitOperation_FullMethodName, in, out, cOpts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

// ReplicationServiceServer is the server API for ReplicationService service.
// All implementations must embed UnimplementedReplicationServiceServer
// for forward compatibility.
type ReplicationServiceServer interface {
	ReplicateOperation(context.Context, *Accept) (*Ack, error)
	CommitOperation(context.Context, *Commit) (*Empty, error)
	mustEmbedUnimplementedReplicationServiceServer()
}

// UnimplementedReplicationServiceServer must be embedded to have
// forward compatible implementations.
//
// NOTE: this should be embedded by value instead of pointer to avoid a nil
// pointer dereference when methods are called.
type UnimplementedReplicationServiceServer struct{}

func (UnimplementedReplicationServiceServer) ReplicateOperation(context.Context, *Accept) (*Ack, error) {
	return nil, status.Errorf(codes.Unimplemented, "method ReplicateOperation not implemented")
}
func (UnimplementedReplicationServiceServer) CommitOperation(context.Context, *Commit) (*Empty, error) {
	return nil, status.Errorf(codes.Unimplemented, "method CommitOperation not implemented")
}
func (UnimplementedReplicationServiceServer) mustEmbedUnimplementedReplicationServiceServer() {}
func (UnimplementedReplicationServiceServer) testEmbeddedByValue()                            {}

// UnsafeReplicationServiceServer may be embedded to opt out of forward compatibility for this service.
// Use of this interface is not recommended, as added methods to ReplicationServiceServer will
// result in compilation errors.
type UnsafeReplicationServiceServer interface {
	mustEmbedUnimplementedReplicationServiceServer()
}

func RegisterReplicationServiceServer(s grpc.ServiceRegistrar, srv ReplicationServiceServer) {
	// If the following call pancis, it indicates UnimplementedReplicationServiceServer was
	// embedded by pointer and is nil.  This will cause panics if an
	// unimplemented method is ever invoked, so we test this at initialization
	// time to prevent it from happening at runtime later due to I/O.
	if t, ok := srv.(interface{ testEmbeddedByValue() }); ok {
		t.testEmbeddedByValue()
	}
	s.RegisterService(&ReplicationService_ServiceDesc, srv)
}

func _ReplicationService_ReplicateOperation_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(Accept)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(ReplicationServiceServer).ReplicateOperation(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: ReplicationService_ReplicateOperation_FullMethodName,
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(ReplicationServiceServer).ReplicateOperation(ctx, req.(*Accept))
	}
	return interceptor(ctx, in, info, handler)
}

func _ReplicationService_CommitOperation_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(Commit)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(ReplicationServiceServer).CommitOperation(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: ReplicationService_CommitOperation_FullMethodName,
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(ReplicationServiceServer).CommitOperation(ctx, req.(*Commit))
	}
	return interceptor(ctx, in, info, handler)
}

// ReplicationService_ServiceDesc is the grpc.ServiceDesc for ReplicationService service.
// It's only intended for direct use with grpc.RegisterService,
// and not to be introspected or modified (even as a copy)
var ReplicationService_ServiceDesc = grpc.ServiceDesc{
	ServiceName: "server.ReplicationService",
	HandlerType: (*ReplicationServiceServer)(nil),
	Methods: []grpc.MethodDesc{
		{
			MethodName: "ReplicateOperation",
			Handler:    _ReplicationService_ReplicateOperation_Handler,
		},
		{
			MethodName: "CommitOperation",
			Handler:    _ReplicationService_CommitOperation_Handler,
		},
	},
	Streams:  []grpc.StreamDesc{},
	Metadata: "proto/server.proto",
}

```
[END FILE: proto/server_grpc.pb.go]


[BEGIN FILE: results/main.go]
```text
package main

import (
"bufio"
"fmt"
"os"
"strings"
)

// LogEntry represents a parsed log line.
type LogEntry struct {
Op string
Start string
End string
Key string
Value string
Duration string
}

func parseLine(line string) (*LogEntry, error) {
parts := strings.Split(line, ",")
if len(parts) != 6 {
return nil, fmt.Errorf("invalid line format: %s", line)
}
return &LogEntry{
Op: parts[0],
Start: parts[1],
End: parts[2],
Key: parts[3],
Value: parts[4],
Duration: parts[5],
}, nil
}

func main() {
	if len(os.Args) < 2 {
	fmt.Println("Usage: go run main.go <logfile>")
	return
	}
	file, err := os.Open(os.Args[1])
	if err != nil {
		fmt.Printf("Error opening file: %v\n", err)
		return
	}
	defer file.Close()

	latestWrites := make(map[string]string)
	scanner := bufio.NewScanner(file)
	lineNum := 0
	totalMis := 0

	for scanner.Scan() {
		lineNum++
		line := strings.TrimSpace(scanner.Text())
		if line == "" {
			continue
		}

		entry, err := parseLine(line)
		if err != nil {
			fmt.Printf("[Line %d] Parse error: %v\n", lineNum, err)
			continue
		}

		switch entry.Op {
		case "WRITE":
			// Record the latest write value for this key
			latestWrites[entry.Key] = entry.Value

		case "READ":
			// Check against last written value if available
			if val, ok := latestWrites[entry.Key]; ok {
				if val != entry.Value {
					fmt.Printf("⚠️  MISMATCH on key '%s' at %s: expected '%s', got '%s'\n",
						entry.Key, entry.Start, val, entry.Value)
					totalMis++
				}
			} else {
				fmt.Printf("ℹ️  READ before any WRITE for key '%s' (value='%s') at %s\n",
					entry.Key, entry.Value, entry.Start)
			}
		default:
			fmt.Printf("[Line %d] Unknown operation: %s\n", lineNum, entry.Op)
		}
	}

	if err := scanner.Err(); err != nil {
		fmt.Printf("Error reading file: %v\n", err)
	}
	fmt.Printf("Total mismatches: %d\n", totalMis)
}
```
[END FILE: results/main.go]


[BEGIN FILE: server/main.go]
```text
package main

import (
	"context"
	"encoding/json"
	"flag" // For testing purposes from chat, so I can distinguish on my local machine the 2 servers
	"log"
	"net"
	"net/http"
	"sync"
	"runtime"
	"time"
	"container/heap"

	pb "project_1/proto"

	"google.golang.org/grpc"
	"google.golang.org/grpc/credentials/insecure"
)

// Chat kept bitching to use a struct instead of leaving these as global variables so I decided to go with it eventually.
// I guess chat is good for trying to make code look neat and with good programming practices
// In the end I think it is easier to compartimentalize my code with this structure, so I'll have to give it to chat here
// But yeah hopefully this is an appropriate use of chat
// (also if I don't do this chat keeps defaulting to using this so its easier to see its suggestions than trying to translate my previous way with global variables.)
type server struct {
	pb.UnimplementedReplicationServiceServer
	hTTPport string // Decided to use the httpport as the ID that the spec wants
	gRPCport string
	is_primary bool
	data_store map[string]string
	lsn int64
	command_log map[int64]*pb.Operation
	addr []string
	conns []*grpc.ClientConn
	servers []pb.ReplicationServiceClient

	// This is probably the most important part of the code that Chat has helped me with (besides getting started in the first place)
	// I've realized that mutexes and ensuring we don't have race conditions is probably necessary to make sure that we have
	// mutual exclusion in critical parts of code. At least I think that's good, although ig we can just wait forever until
	// our values are applied. But mutexes are still good to make sure that the values are applied in order if we have writes
	// and reads to the same value. So I guess in the end this is just a good idea.
	mu   sync.Mutex
	cond *sync.Cond
	commitHeap *CommitHeap
	completedLsn int64
}

type CommitHeap []*pb.Commit

func (h CommitHeap) Len() int           { return len(h) }
func (h CommitHeap) Less(i, j int) bool { return h[i].Lsn < h[j].Lsn } // smallest LSN first
func (h CommitHeap) Swap(i, j int)      { h[i], h[j] = h[j], h[i] }

func (h *CommitHeap) Push(x any) {
    *h = append(*h, x.(*pb.Commit))
}

func (h *CommitHeap) Pop() any {
    old := *h
    n := len(old)
    x := old[n-1]
    *h = old[:n-1]
    return x
}

var totalReads = 0
var totalWrites = 0
var debug = true

// Chat has made me aware that I need to actually do an HTTP server, not the gRPC
func (s *server) Read(w http.ResponseWriter, r *http.Request) {
	key := r.URL.Path[1:]
	if debug {
		totalReads += 1
		log.Printf("total reads: %d", totalReads)
		log.Printf("Received READ request for key %s: ", key)
	}

	if s.is_primary {
		op := pb.Operation {
			Key: key,
			Val: "",
			IsRead: true,
			Applied: false,
		}
		commit := s.replicateToBackups(&op)
		s.CommitOperation(context.Background(), commit)
		s.mu.Lock()
		for !s.command_log[commit.Lsn].Applied { // Wait for applied
			s.cond.Wait()
		}
		s.mu.Unlock()
		s.CommitToBackups(commit)

		if debug {
			log.Printf("DEBUG: finishing the read")
		}
	}

	s.mu.Lock()
	value, ok := s.data_store[key]
	s.mu.Unlock()

	if debug {
		log.Printf("DEBUG: After broadcast")
	}
	if ok {
		if debug {
			log.Printf("DEBUG: Found it")
		}
		resp := map[string]string{"key": key, "value": value}
		json.NewEncoder(w).Encode(resp)
	} else {
		if debug {
		log.Printf("DEBUG: Didnt find it")
		}
		http.Error(w, "not found", http.StatusNotFound)
		return
	}
}

func (s *server) Write(w http.ResponseWriter, r *http.Request) {
	if !s.is_primary {
		http.Error(w, "cannot write to backup", http.StatusNotFound)
		return
	}
	key := r.URL.Path[1:]
	val := r.FormValue("val")
	if debug {
		totalWrites++
		log.Printf("total writes: %d", totalWrites)
		log.Printf("Received WRITE request for key %s value %s: ", key, val)
	}
	if val == "" {
		http.Error(w, "missing val field", http.StatusBadRequest)
		return
	}

	op := pb.Operation {
		Key: key,
		Val: val,
		IsRead: false,
		Applied: false,
	}
	commit := s.replicateToBackups(&op)
	s.CommitOperation(context.Background(), commit)
	s.mu.Lock()
	for !s.command_log[commit.Lsn].Applied { // Wait for applied
		s.cond.Wait()
	}
	s.mu.Unlock()
	s.CommitToBackups(commit)

	resp := map[string]string{"status": "ok"}
	json.NewEncoder(w).Encode(resp)
}

// This is from chat as trying to figure out how to do the quorum majority.
// I think the main line is the ack, err := backupClient line, as the context.Background() is the main thing that is needed for concurrency here.
func (s *server) replicateToBackups(op *pb.Operation) *pb.Commit {
	// This is standard for quorum formula
	quorum := len(s.servers) / 2 + 1
	acks := 1
	
	// Ok fuck chat i think imma just code myself i got this. I think this is the way to go about concurrency
	// but making sure everything is in order. The most important thing is that we do NOT have duplicate LSNs.
	// So I just need to make it so that we lock only the lsn increment and setting the accept for the backups
	// to get that accept request. I think this logically makes sense, it'll allow concurrent requests but
	// make sure there is never a duplicate LSN. I think we're not worried about error, just let it wait forever
	s.mu.Lock()
	accept := pb.Accept {
		Op: op,
		Lsn: s.lsn,
	}
	s.command_log[s.lsn] = op
	commit := pb.Commit {
		Lsn: s.lsn,
	}
	if debug {
		log.Printf("Debug LSN Number: %d\n", s.lsn)
	}
	s.lsn++
	s.mu.Unlock()

	ackCh := make(chan bool, len(s.servers))

	// Going through every backup
	for _, server := range s.servers {
		go func(server pb.ReplicationServiceClient) {
			// This is the important part that tells every backup to replicate the operation
			ack, err := server.ReplicateOperation(context.Background(), &accept)
			ackCh <- (err == nil && ack != nil && ack.Success)
		}(server)
	}

	for {
		if <-ackCh {
				acks++
		}
		if acks >= quorum {
			if debug {
				log.Println("DEBUG: weve got quorum")
			}
			return &commit // quorum reached, return immediately
		}
	}
}


// I took chat's original implementation and modified it to the workflow defined in the specs
// The function header was assisted by chat (for syntax), but then I figured out how to do the rest myself.
// I'm realizing go really likes them pointers (or perhaps the way chat wanted me to do it does it that way)
// Anyway, a lot of the work is just me figuring out how to get the correct types passed and returned.
func (s *server) ReplicateOperation(ctx context.Context, accept *pb.Accept) (*pb.Ack, error) {
	s.mu.Lock()
	s.command_log[accept.Lsn] = accept.Op
	s.mu.Unlock()
	s.cond.Broadcast()
	if debug {
		log.Printf("DEBUG: replicating operation\n")
	}

	return &pb.Ack {
		Id: s.hTTPport,
		Lsn: accept.Lsn,
		Success: true,
	}, nil
}

func (s *server) CommitToBackups(commit *pb.Commit) {
	if debug {
		log.Printf("DEBUG: Committing to backups")
	}
	for _, server := range s.servers {
		go func(server pb.ReplicationServiceClient) {
			server.CommitOperation(context.Background(), commit)
		}(server)
	}
}

func (s *server) CommitOperation(ctx context.Context, commit *pb.Commit) (*pb.Empty, error) {
	s.mu.Lock()
	heap.Push(s.commitHeap, commit)
	s.cond.Broadcast() // wake up the applier
	s.mu.Unlock()
	if debug {
		log.Printf("DEBUG: commit operation: %d\n", commit.Lsn)
	}
	return &pb.Empty{}, nil
}

// This is copied from program 0
func startServer(httpport string, grpcport string, primary bool, backups []string) {
	// Chat's thing for initializing our new server so that we can hold our server data
	srv := &server{
		hTTPport:         httpport,
		gRPCport: 			grpcport,
		is_primary:  primary,
		lsn:        0,
		data_store:  make(map[string]string),
		command_log: make(map[int64]*pb.Operation),
		addr:  backups,
    conns:       []*grpc.ClientConn{},
    servers:     []pb.ReplicationServiceClient{},
		completedLsn: 1,
	}
	srv.commitHeap = &CommitHeap{}
	heap.Init(srv.commitHeap)
	srv.command_log[0] = &pb.Operation{Applied: true}
	srv.lsn = 1
	// This is the part where we actually initialize the chat suggested mutexes and conds.
	srv.cond = sync.NewCond(&srv.mu)
	log.Printf("Server listening on HTTP port %s gRPC port %s (primary=%v)", httpport, grpcport, primary)
	go func() {
    for {
        log.Printf("Number of goroutines: %d", runtime.NumGoroutine())
				time.Sleep(10 * time.Second)
    }
	}()
	go func() { // Function to constantly process commits
		for {
			srv.mu.Lock()
			for srv.commitHeap.Len() == 0 || (*(srv.commitHeap))[0].Lsn != srv.completedLsn {
					srv.cond.Wait()
			}
			commit := heap.Pop(srv.commitHeap).(*pb.Commit)

			op, ok := srv.command_log[commit.Lsn]
			for !ok || op == nil {
				log.Printf("Heyy")
				srv.cond.Wait()
				op, ok = srv.command_log[commit.Lsn]
			}
			if !op.IsRead {
				if debug {
					log.Printf("Applying key %s value %s to HTTP %s", op.Key, op.Val, srv.hTTPport)
				}
				srv.data_store[op.Key] = op.Val
			}
			srv.command_log[commit.Lsn].Applied = true
			srv.completedLsn++
			srv.cond.Broadcast()
			srv.mu.Unlock()
		}
	}()
	if (srv.is_primary) {
		// So last time this was where I lost most of my points, cuz I kept connecting and disconnecting.
		// Anyways I wanted to know if the conn itself or the New_ServiceClient would be the majority of the computation.
		// Chat says that the conn itself is most as that is the actual gRPC connection, which I think does make sense.
		// Anyways I didn't actually chat this code, just needed clarification. I'm just writing to make sure I'm not cheating
		// This is mostly copied from program_0 and then modified for my needs this time.
		for _, backup := range backups {
			conn, err := grpc.NewClient(backup, grpc.WithTransportCredentials(insecure.NewCredentials()))
			if err != nil {
				log.Fatalf("Failed to connect: %v", err)
			}
			srv.conns = append(srv.conns, conn)
			srv.servers = append(srv.servers, pb.NewReplicationServiceClient(conn))
		}
	} else {
		go func() {
			grpcServer := grpc.NewServer() // Creates a gRPC server object that can use the gRPC protocol
			pb.RegisterReplicationServiceServer(grpcServer, srv)
			lis, err := net.Listen("tcp", "127.0.0.1:" + grpcport)

			if err != nil { // Error checking, don't even need ChatGPT to understand this now, im a pro
				log.Fatalf("Failed to listen: %v", err)
			}
			if err := grpcServer.Serve(lis); err != nil {
				log.Fatalf("Failed to serve: %v", err)
			}
		}()
	}

	// Chat's thing for HTTP client-facing
	http.HandleFunc("/", func(w http.ResponseWriter, r *http.Request) {
		switch r.Method {
		case http.MethodGet:
			srv.Read(w, r)
		case http.MethodPost:
			srv.Write(w, r)
		default:
			http.Error(w, "unsupported method", http.StatusMethodNotAllowed)
		}
	})
	log.Fatal(http.ListenAndServe("127.0.0.1:" + httpport, nil))
}

func main() {
	httpport := flag.String("http", "8080", "http server port")
	grpcport := flag.String("grpc", "50051", "grpc server port")
	primary := flag.Bool("primary", false, "is primary node")
	backups := []string{"127.0.0.1:50052", "127.0.0.1:50053"} // Chat suggested putting this here, ig its better for programming practice to put it in an easy spot to modify
	flag.Parse()
	startServer(*httpport, *grpcport, *primary, backups)
}
```
[END FILE: server/main.go]


[BEGIN FILE: server/main_old.go]
```text
package main

import (
	"context"
	"encoding/json"
	"flag" // For testing purposes from chat, so I can distinguish on my local machine the 2 servers
	"log"
	"net"
	"net/http"
	"sync"
	"time"
	"runtime"

	pb "project_1/proto"

	"google.golang.org/grpc"
	"google.golang.org/grpc/credentials/insecure"
)

// Chat kept bitching to use a struct instead of leaving these as global variables so I decided to go with it eventually.
// I guess chat is good for trying to make code look neat and with good programming practices
// In the end I think it is easier to compartimentalize my code with this structure, so I'll have to give it to chat here
// But yeah hopefully this is an appropriate use of chat
// (also if I don't do this chat keeps defaulting to using this so its easier to see its suggestions than trying to translate my previous way with global variables.)
type server struct {
	pb.UnimplementedReplicationServiceServer
	hTTPport string // Decided to use the httpport as the ID that the spec wants
	gRPCport string
	is_primary bool
	data_store map[string]string
	lsn int64
	command_log map[int64]*pb.Operation
	addr []string
	conns []*grpc.ClientConn
	servers []pb.ReplicationServiceClient

	// This is probably the most important part of the code that Chat has helped me with (besides getting started in the first place)
	// I've realized that mutexes and ensuring we don't have race conditions is probably necessary to make sure that we have
	// mutual exclusion in critical parts of code. At least I think that's good, although ig we can just wait forever until
	// our values are applied. But mutexes are still good to make sure that the values are applied in order if we have writes
	// and reads to the same value. So I guess in the end this is just a good idea.
	mu   sync.Mutex
	cond *sync.Cond
}

// Chat has made me aware that I need to actually do an HTTP server, not the gRPC
func (s *server) Read(w http.ResponseWriter, r *http.Request) {
	key := r.URL.Path[1:]
	log.Printf("Received READ request for key %s: ", key)

	if s.is_primary {
		op := pb.Operation {
			Key: key,
			Val: "",
			IsRead: true,
			Applied: false,
		}
		commit := s.replicateToBackups(&op)
		s.CommitToBackups(commit)
		
		s.mu.Lock()
		defer s.mu.Unlock()

		log.Printf("DEBUG: finishing the read")
		prev_op, ok := s.command_log[commit.Lsn - 1]
		for !ok || prev_op == nil || !prev_op.Applied {
				s.cond.Wait()
		}
		s.command_log[commit.Lsn].Applied = true
	} else {
		s.mu.Lock()
		defer s.mu.Unlock()
	}
	value, ok := s.data_store[key]
	if s.is_primary {
		s.cond.Broadcast()
	}

	log.Printf("DEBUG: After broadcast")
	if ok {
		resp := map[string]string{"key": key, "value": value}
		json.NewEncoder(w).Encode(resp)
	} else {
		http.Error(w, "not found", http.StatusNotFound)
		return
	}
}

func (s *server) Write(w http.ResponseWriter, r *http.Request) {
	key := r.URL.Path[1:]
	val := r.FormValue("val")
	log.Printf("Received WRITE request for key %s value %s: ", key, val)
	if val == "" {
		http.Error(w, "missing val field", http.StatusBadRequest)
		return
	}

	op := pb.Operation {
		Key: key,
		Val: val,
		IsRead: false,
		Applied: false,
	}
	commit := s.replicateToBackups(&op)
	s.CommitToBackups(commit)
	
	s.mu.Lock()
	defer s.mu.Unlock()

	prev_op, ok := s.command_log[commit.Lsn - 1]
	for !ok || prev_op == nil || !prev_op.Applied {
			s.cond.Wait()
	}
	s.data_store[key] = val
	s.command_log[commit.Lsn].Applied = true
	s.cond.Broadcast()

	resp := map[string]string{"status": "ok"}
	json.NewEncoder(w).Encode(resp)
}

// This is from chat as trying to figure out how to do the quorum majority.
// I think the main line is the ack, err := backupClient line, as the context.Background() is the main thing that is needed for concurrency here.
func (s *server) replicateToBackups(op *pb.Operation) *pb.Commit {
	// This is standard for quorum formula
	quorum := len(s.servers) / 2 + 1
	acks := 1
	
	// Ok fuck chat i think imma just code myself i got this. I think this is the way to go about concurrency
	// but making sure everything is in order. The most important thing is that we do NOT have duplicate LSNs.
	// So I just need to make it so that we lock only the lsn increment and setting the accept for the backups
	// to get that accept request. I think this logically makes sense, it'll allow concurrent requests but
	// make sure there is never a duplicate LSN. I think we're not worried about error, just let it wait forever
	s.mu.Lock()
	s.lsn++
	accept := pb.Accept {
		Op: op,
		Lsn: s.lsn,
	}
	s.command_log[s.lsn] = op
	commit := pb.Commit {
		Lsn: s.lsn,
	}
	log.Printf("Debug LSN Number: %d\n", s.lsn)
	s.mu.Unlock()

	ackCh := make(chan bool, len(s.servers))

	// Going through every backup
	for _, server := range s.servers {
		go func(server pb.ReplicationServiceClient) {
			// This is the important part that tells every backup to replicate the operation
			ack, err := server.ReplicateOperation(context.Background(), &accept)
			if err != nil {
					log.Printf("DEBUG: ReplicateOperation failed: %v", err)
					ackCh <- false
					return
			}
			if ack == nil {
					log.Println("DEBUG: ack is nil")
					ackCh <- false
					return
			}			
			if !ack.Success {
					log.Println("DEBUG: yikes it failed")	
					ackCh <- false
					return
			}
			if ack.Success {
					log.Println("DEBUG: got quorum")	
					ackCh <- true
					return
			}
		}(server)
	}

	for {
		if <-ackCh {
				acks++
		}
		if acks >= quorum {
				log.Println("DEBUG: weve got quorum")
				return &commit // quorum reached, return immediately
		}
	}
}


// I took chat's original implementation and modified it to the workflow defined in the specs
// The function header was assisted by chat (for syntax), but then I figured out how to do the rest myself.
// I'm realizing go really likes them pointers (or perhaps the way chat wanted me to do it does it that way)
// Anyway, a lot of the work is just me figuring out how to get the correct types passed and returned.
func (s *server) ReplicateOperation(ctx context.Context, accept *pb.Accept) (*pb.Ack, error) {
	s.mu.Lock()
	s.command_log[accept.Lsn] = accept.Op
	s.mu.Unlock()
	s.cond.Broadcast()
	log.Printf("DEBUG: replicating operation\n")

	return &pb.Ack {
		Id: s.hTTPport,
		Lsn: accept.Lsn,
		Success: true,
	}, nil
}

func (s *server) CommitToBackups(commit *pb.Commit) {
	log.Printf("DEBUG: Committing to backups")
	for _, server := range s.servers {
		server.CommitOperation(context.Background(), commit)
	}
}

func (s *server) CommitOperation(ctx context.Context, commit *pb.Commit) (*pb.Empty, error) {
	log.Printf("DEBUG: commit operation: %d\n", commit.Lsn)
	// Continuing on with Chat, this is the blocking mechanism suggested.
	s.mu.Lock()
	defer s.mu.Unlock()

	// wait until this LSN is available (This is specifically the part in the spec that wants us to wait forever)
	// Ok I understand that s.cond.Wait() will indeed release the mutex, so this will not block the process forever
  // Ok I went back and forth here why we want this condition for the mutex (s.lsn + 1 instead of commit.lsn) as both do work
	// But commit.lsn will force each goroutine to go one by one to wake up, wherras we can just wake up all of the
	// commits that are out of order simultanenously, and then just check the hash map to make sure the values are done in
	// order/
	prev_op, ok := s.command_log[commit.Lsn - 1]
	for !ok || prev_op == nil || !prev_op.Applied {
		s.cond.Wait()
		prev_op, ok = s.command_log[commit.Lsn - 1]
	}
	log.Printf("DEBUG: past the previous\n")

	op, ok := s.command_log[commit.Lsn]
	for !ok || op == nil {
		s.cond.Wait()
		op, ok = s.command_log[commit.Lsn]
	}

	s.command_log[commit.Lsn].Applied = true
	if !op.IsRead {
		s.data_store[op.Key] = op.Val
	}
	log.Printf("DEBUG: successful commit operation: %d\n", commit.Lsn)
	// This does feel like CS620, like I feel like I remember seeing this condition exist
	// Would mutexes be mainly used for cloud computing? Ig there's other stuff that needs concurrency
	// But this does feel like one of the biggest use cases
	s.cond.Broadcast()
	return &pb.Empty{}, nil
}

// This is copied from program 0
func startServer(httpport string, grpcport string, primary bool, backups []string) {
	// Chat's thing for initializing our new server so that we can hold our server data
	srv := &server{
		hTTPport:         httpport,
		gRPCport: 			grpcport,
		is_primary:  primary,
		lsn:        0,
		data_store:  make(map[string]string),
		command_log: make(map[int64]*pb.Operation),
		addr:  backups,
    conns:       []*grpc.ClientConn{},
    servers:     []pb.ReplicationServiceClient{},
	}
	srv.command_log[0] = &pb.Operation{Applied: true}
	// This is the part where we actually initialize the chat suggested mutexes and conds.
	srv.cond = sync.NewCond(&srv.mu)
	log.Printf("Server listening on HTTP port %s gRPC port %s (primary=%v)", httpport, grpcport, primary)
	go func() {
    for {
        log.Printf("Number of goroutines: %d", runtime.NumGoroutine())
				time.Sleep(10 * time.Second)
    }
	}()

	if (srv.is_primary) {
		// So last time this was where I lost most of my points, cuz I kept connecting and disconnecting.
		// Anyways I wanted to know if the conn itself or the New_ServiceClient would be the majority of the computation.
		// Chat says that the conn itself is most as that is the actual gRPC connection, which I think does make sense.
		// Anyways I didn't actually chat this code, just needed clarification. I'm just writing to make sure I'm not cheating
		// This is mostly copied from program_0 and then modified for my needs this time.
		for _, backup := range backups {
			conn, err := grpc.NewClient(backup, grpc.WithTransportCredentials(insecure.NewCredentials()))
			if err != nil {
				log.Fatalf("Failed to connect: %v", err)
			}
			srv.conns = append(srv.conns, conn)
			srv.servers = append(srv.servers, pb.NewReplicationServiceClient(conn))
		}
	} else {
		go func() {
			grpcServer := grpc.NewServer() // Creates a gRPC server object that can use the gRPC protocol
			pb.RegisterReplicationServiceServer(grpcServer, srv)
			lis, err := net.Listen("tcp", "127.0.0.1:" + grpcport)

			if err != nil { // Error checking, don't even need ChatGPT to understand this now, im a pro
				log.Fatalf("Failed to listen: %v", err)
			}
			if err := grpcServer.Serve(lis); err != nil {
				log.Fatalf("Failed to serve: %v", err)
			}
		}()
	}

	// Chat's thing for HTTP client-facing
	http.HandleFunc("/", func(w http.ResponseWriter, r *http.Request) {
		switch r.Method {
		case http.MethodGet:
			srv.Read(w, r)
		case http.MethodPost:
			srv.Write(w, r)
		default:
			http.Error(w, "unsupported method", http.StatusMethodNotAllowed)
		}
	})
	log.Fatal(http.ListenAndServe("127.0.0.1:" + httpport, nil))
}

func main() {
	httpport := flag.String("http", "8080", "http server port")
	grpcport := flag.String("grpc", "50051", "grpc server port")
	primary := flag.Bool("primary", false, "is primary node")
	backups := []string{"127.0.0.1:50052", "127.0.0.1:50053"} // Chat suggested putting this here, ig its better for programming practice to put it in an easy spot to modify
	flag.Parse()
	startServer(*httpport, *grpcport, *primary, backups)
}

```
[END FILE: server/main_old.go]
